'''
Created on Mar 21, 2013

@author: larsmaaloee
'''
from nltk.tokenize import wordpunct_tokenize
#from nltk.stem.snowball import EnglishStemmer
from nltk.stem.porter import PorterStemmer as EnglishStemmer
import numpy as np
from numpy import *
import numpy.random as nr
import os
from nltk.corpus import stopwords
import re
import string
import cPickle as pickle
from time import time
from itertools import chain
import env_paths
from random import shuffle
from multiprocessing import Pool
import random

class DataProcessing(object):
    '''
    Open all documents from a specified path.
    Generate a word matrix.
    
    This class only works on English documents.
    '''

    def __init__(self, paths, words_count = 2000, batchsize = 100, trainingset_size = None, validation_set_words_list = None, acceptance_lst_path = None):
        '''
        Open all documents from the given path.
        Initialize the variables needed in order
        to construct the word matrix.
        
        Parameters
        ----------
        paths: paths to the documents.
        words_count: number of words in the bag of words.
        trainingset_size: the proportion of the data that should be set to the training set.
        validation_set_words_list: the attributes for validation.
        '''


        # Set paths
        if trainingset_size == None:
            self.training = True
        elif not trainingset_size == None and validation_set_words_list == None:
            self.training = True
        else:
            self.training = False

        # If the acceptance list is given.
        if acceptance_lst_path == None:
            self.acceptance_lst = None
        else:
            self.acceptance_lst = open(acceptance_lst_path).read().replace(" ","").split("\n")

        self.max_words_matrix = words_count

        print 'Data Processing Started'

        timer = time()
        self.__read_docs_from_filesystem(paths, batchsize, trainingset_size,validation_set_words_list)
        print 'Time ',time()-timer

        print 'Filtering Words'
        timer = time()

        # Add all text of docs as a tokenized list
        if validation_set_words_list == None:
            attributes = self.__set_attributes()
        else:
            attributes = validation_set_words_list
            pickle.dump(attributes, open(env_paths.get_attributes_path(self.training), "wb"))

        print 'Time ',time()-timer

        print 'Generate bag of words matrix'
        timer = time()
        # Generate a dictionary for lookup of the words
        index_lookup = dict(zip(attributes,range(len(attributes))))
        # Generate word matrix
        self.__generate_word_matrix(index_lookup)

        print 'Time ',time()-timer


    def __read_docs_from_filesystem(self,paths, batchsize, trainingset_size,validation_set_words_list):
        # Holds the names of the documents.
        docs_names = []
        docs_names_split = []
        # The indices for each class for classification purposes
        class_indices = []
        class_indices_split = []
        # Class names in the dataset used for training.
        class_names = []
        # List containing the number of each batch
        batches = []
        print 'Generating class indices and docs names list.'
        doc_count = 0
        for folder in paths:
            docs_names_split.append([])
            class_indices_split.append([])
            class_names.append(folder.split('/')[len(folder.split('/'))-1])
            if trainingset_size == None: # If data processing should be done on all data in the specified folders.
                docs = os.listdir(folder)
            elif not trainingset_size == None and validation_set_words_list == None: # If data processing should be done on parts of the docs in the specified folders - for training and testing purposes.
                docs = os.listdir(folder)[:int(len(os.listdir(folder))*trainingset_size)]
            else: # If data processing should be done on a test set for validation of the model.
                docs = os.listdir(folder)[int(len(os.listdir(folder))*trainingset_size):]
            length = len(docs)
            for doc in docs:
                if doc.endswith('.p'):
                    # Append the name of the document to the list containing document names.
                    #docs_names.append(folder+'/'+doc)
                    # Increase the class indices counter.
                    #class_indices.append(len(class_names)-1)

                    docs_names_split[-1].append(folder+'/'+doc)
                    class_indices_split[-1].append(len(class_names)-1)
                    doc_count += 1



        if len(docs_names_split) == 0: # Check if docs have been stemmed.
            print 'Documents have not been stemmed. Please stem documents in order to create bag of words matrices.'
            return


        # Arrange docs
        print 'Arranging the documents.'
        number_of_batches = doc_count/batchsize
        number_of_classes = len(paths)
        batches_collected_class_indices = []
        batches_collected_docs_names = []


        d = {}
        for i in range(len(class_indices_split)):
            d[i] = float(len(class_indices_split[i]))/number_of_batches

        count = 0
        for i in range(number_of_batches):
            batch_class_indices = []
            batch_docs_names = []
            d_tmp = np.array([int(v) for v in d.values()])
            while True:
                if (len(batch_class_indices) == batchsize) and (not doc_count-count < batchsize) or (count == doc_count):
                        break
                if len(d_tmp[d_tmp>0]) == 0:
                    break

                for j in range(number_of_classes):
                    if (len(batch_class_indices) == batchsize) and (not doc_count-count < batchsize) or (count == doc_count):
                        break
                    if len(class_indices_split[j]) > 0 and d_tmp[j] != 0:
                        batch_class_indices.append(class_indices_split[j].pop(0))
                        batch_docs_names.append(docs_names_split[j].pop(0))
                        d_tmp[j] -= 1
                        count += 1


            batches_collected_class_indices.append(batch_class_indices)
            batches_collected_docs_names.append(batch_docs_names)

        for i in range(number_of_batches):
            bsize = batchsize if i < number_of_batches-1 else batchsize + (doc_count % batchsize)
            batch_class_indices = batches_collected_class_indices[i]
            batch_docs_names = batches_collected_docs_names[i]
            if len(batch_class_indices) < bsize:
                while True:
                    if len(batch_class_indices) == bsize: break
                    for j in range(number_of_classes):
                        if len(batch_class_indices) == bsize: break
                        if len(class_indices_split[j]) > 0:
                            batch_class_indices.append(class_indices_split[j].pop(0))
                            batch_docs_names.append(docs_names_split[j].pop(0))


            # Shuffle the batch
            batch_class_indices_shuf = []
            batch_docs_names_shuf = []
            index_shuf = range(len(batch_class_indices))
            shuffle(index_shuf)
            for k in index_shuf:
                batch_class_indices_shuf.append(batch_class_indices[k])
                batch_docs_names_shuf.append(batch_docs_names[k])

            # Append batch to full lists
            class_indices += batch_class_indices_shuf
            docs_names += batch_docs_names_shuf


        #print 'Shuffle lists'
        #class_indices_shuf = []
        #docs_names_shuf = []
        #index_shuf = range(len(class_indices))
        #shuffle(index_shuf)
        #for i in index_shuf:
        #    class_indices_shuf.append(class_indices[i])
        #    docs_names_shuf.append(docs_names[i])

        print 'Reading and saving docs from file system'
        count = 0
        class_indices_batch = []
        docs_names_batch = []
        docs_list = []
        for i in xrange(len(class_indices)):
            if not count == 0 and (count % batchsize) == 0: # Save the batch if batchsize is reached or if the last document has been read.
                if not (len(class_indices) - count) < batchsize:
                    print 'Read ',str(count),' of ',len(class_indices)
                    self.__save_batch_loading_docs(count,docs_list,docs_names_batch,class_indices_batch)
                    batches.append(count)
                    # Reset the lists
                    docs_list = []
                    docs_names_batch = []
                    class_indices_batch = []

            d = pickle.load(open(docs_names[i],'rb'))
            # Append a filtered version of the document to the document list.
            docs_list.append(d)
            # Append the name of the document to the list containing document names.
            docs_names_batch.append(docs_names[i])
            # Increase the class indices counter.
            class_indices_batch.append(class_indices[i])
            count += 1

        # Save the remaining docs
        if len(docs_list) > 0:
            print 'Read ',str(count),' of ',len(class_indices)
            self.__save_batch_loading_docs(count,docs_list,docs_names_batch,class_indices_batch)
            batches.append(count)

        pickle.dump(class_names , open( env_paths.get_class_names_path(self.training), "wb" ) )
        pickle.dump(batches, open(env_paths.get_batches_path(self.training), "wb"))


    def __set_attributes(self):
        '''
        Set the attributes containing of a list of words of all attributes
        in the bag of words matrix.

        Parameters
        ----------
        path_load: the path where the document lists should be loaded from.
        path_dump: the path where the attributes should be dumped.
        '''
        batches = pickle.load( open( env_paths.get_batches_path(self.training), "rb" ) )
        length = len(batches)
        attributes = []
        processed = 1
        for batch in batches:
            docs_list = pickle.load( open( env_paths.get_doc_list_path(self.training,batch), "rb" ) )

            tmp_attributes = list(set(sorted(list(chain(*docs_list))))) # Retrieve the each word of the docs list in a sorted list
            attributes += tmp_attributes
            attributes = list(set(sorted(attributes))) # Sort the attributes list so that there is no 2 occurrences of the same word.
            if not self.acceptance_lst == None: attributes = list(set(attributes).intersection(self.acceptance_lst)) # Only consider words in the acceptance list.
            print 'Processed attribute '+str(processed)+' of '+str(length)+' batches'
            processed += 1

        # Find attributes of the most common words.
        d = dict.fromkeys(attributes)
        processed = 1
        for batch in batches:
            docs_list = pickle.load( open( env_paths.get_doc_list_path(self.training,batch), "rb" ) )
            words = list(list(chain(*docs_list)))
            for w in words:
                try:
                    if d[w] == None:
                        d[w] = 1
                    else:
                        d[w] += 1
                except KeyError:
                    continue
            print 'Processed summing '+str(processed)+' of '+str(length)+' batches'
            processed += 1
        sorted_att = sorted(d.items(),key = lambda x: x[1])
        sorted_att = sorted_att[len(sorted_att)-self.max_words_matrix:]
        attributes = [elem[0] for elem in sorted_att]

        # Save attributes to pickle
        pickle.dump(attributes, open(env_paths.get_attributes_path(self.training), "wb"))
        return attributes

    def __save_batch_loading_docs(self,batch_number,docs_list,docs_names,class_indices):
        '''
        Save batches for the document loading process in the
        initialization phase. This is done due to vast sizes of data.

        Parameters
        ----------
        batch_number: Representing the number of documents in the batch.
        docs_list: List containing a string for each document in the batch.
        docs_names: List containing the names of each document in the same order as the docs_list.
        class_indices: List containing which class/folder each document belongs to.
        path_load: The path to store the pickled docs_lists
        path_dump: The path to store the pickled class_names, docs_names, class_indices.
        '''

        # Dump all relevant variables into pickle
        pickle.dump(docs_list , open( env_paths.get_doc_list_path(self.training,batch_number), "wb" ) )
        pickle.dump(docs_names , open( env_paths.get_doc_names_path(self.training,batch_number), "wb" ) )
        pickle.dump(class_indices , open( env_paths.get_class_indices_path(self.training,batch_number), "wb" ) )


    def __generate_word_matrix(self, index_lookup):
        '''
        Generate a word matrix with rows, columns
        corresponding to documents, words respectively.

        Parameters
        ----------
        index_lookup: A dictionary with keys for the attributes. In order to know which colounm should be incremented in word_matrix.
        '''


        batches = pickle.load( open( env_paths.get_batches_path(self.training), "rb" ) )
        length = len(batches)
        processed = 1
        for batch in batches:
            docs_list = pickle.load( open( env_paths.get_doc_list_path(self.training,batch), "rb" ) )
            bag_of_words_matrix = np.zeros([len(docs_list),len(index_lookup)])
            row = 0
            for doc in docs_list:
                for token in doc:
                    try: # If word is not found in the dictionary
                        col = index_lookup[token]
                        bag_of_words_matrix[row,col] += 1
                    except KeyError:
                        continue
                row += 1
            # Save bag of words to pickle
            pickle.dump(bag_of_words_matrix, open(env_paths.get_bow_matrix_path(self.training,batch), "wb"))
            print 'Processed '+str(processed)+' of '+str(length)+' batches'
            processed += 1

def stem_docs(paths):
    '''
    Stem all documents from the given path names

    Parameters
    ----------
    paths: paths to the documents.
    trainingset_size: the proportion of the data that should be set to the training set.
    validation_set_words_list: the attributes for validation.
    '''
    print 'Stemming documents in parallel.'
    docs = []
    for folder in paths:
        d = os.listdir(folder)
        docs += [os.path.join(x,y) for (x,y) in zip([folder for _ in range(len(d))],d)]


    for doc in docs:
        if not doc.endswith(".txt"): docs.remove(doc)
    p = Pool()

def __stem_doc(doc_details):
    idx, doc = doc_details
    if idx % 100 == 0:
        print "Processed doc "+str(idx)
    if doc.endswith('.txt'):
        d = open(doc).read()
        stemmer = EnglishStemmer()# This method only works for english documents.
        attribute_names = [stemmer.stem(token.lower()) for token in wordpunct_tokenize(re.sub('[%s]' % re.escape(string.punctuation), '', d.decode(encoding = 'UTF-8',errors = 'ignore'))) if token.lower() not in stopwords.words('english')]# Stem, lowercase, substitute all punctuations, remove stopwords.
        pickle.dump(attribute_names,open(doc.replace(".txt",".p"),"wb"))


def get_bag_of_words_matrix(batch, training = True):
    '''
    Retrieve the bag of words matrix for a batch.

    Parameters
    ----------
    batch: the number of the batch.
    '''
    return pickle.load( open( env_paths.get_bow_matrix_path(training,int(batch)), "rb" ) )

def get_batch_list(training = True):
    '''
    Retrieve the list containing the batch numbers.

    Parameters
    ----------
    training: is this the training set or the test set.
    '''
    return pickle.load( open( env_paths.get_batches_path(training), "rb" ) )


def get_document_name(row, batch, training = True):
    '''
    The name of the document corresponding to a row
    in a batch.

    Parameters
    ----------
    row: row in the bag of words matrix in batch.
    batch: the number of the batch.
    training: is this the training set or the test set.

    Returns
    ----------
    '''
    return pickle.load( open( env_paths.get_doc_names_path(training,batch), "rb" ) )[row]

def get_document_names(batch,training = True):
    '''
    Get document names.

    Parameters
    ----------
    batch: the number of the batch.
    training: is this the training set or the test set.
    '''
    names = pickle.load( open( env_paths.get_doc_names_path(training,batch), "rb" ) )
    return names

def get_class_indices(batch, training = True):
    '''
    Get all class indices of the documents in a batch.

    Parameters
    ----------
    batch: the number of the batch.
    training: is this the training set or the test set.
    '''

    indices = pickle.load( env_paths.get_class_indices_path(training,batch), "rb" )
    return indices

def get_document_class(row,batch, training = True):
    '''
    The class of a document corresponding to a row
    in a batch.

    Parameters
    ----------
    row: row in the bag of words matrix in batch.
    batch: the number of the batch.
    training: is this the training set or the test set.

    '''
    class_indices_for_batch = pickle.load( open( env_paths.get_class_indices_path(training,batch), "rb" ) )
    class_names_for_batch = pickle.load( open( env_paths.get_class_names_path(training), "rb" ) )
    return class_names_for_batch[class_indices_for_batch[row]]

def get_attributes(training = True):
    '''
    Get the attributes.

    Parameters
    ----------
    training: is this the training set or the test set.

    '''
    return pickle.load( open( env_paths.get_attributes_path(training), "rb" ) )

def get_all_class_indices(training = True):

    batches = get_batch_list(training)
    indices_collected = []

    for batch in batches:
        indices_collected += pickle.load( open( env_paths.get_class_indices_path(training,batch), "rb" ) )

    return indices_collected

def stem_acceptance_list(path):
    acceptance_lst = open(path).read().replace(" ","").split("\n")
    stemmer = EnglishStemmer()
    acceptance_lst_stemmed = []
    for word in acceptance_lst:
        acceptance_lst_stemmed.append(stemmer.stem(word.lower()))

    f = open('output/acceptance_lst_stemmed.txt','w')
    for w in acceptance_lst_stemmed[:-1]:
        f.write(w+"\n")
    f.write(acceptance_lst_stemmed[-1])
    f.close()nce_lst_stemmed.txt','w')
    for w in acceptance_lst_stemmed[:-1]:
        f.write(w+"\n")
    f.write(acceptance_lst_stemmed[-1])
    f.close()

